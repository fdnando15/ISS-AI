{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d3fa240-87fe-4fd6-8779-31c4e6e9c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26dd85e3-2200-494e-8dcc-fe828edabb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('DataFram').getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e1171bf-33e5-4a0a-a06c-50758ae940a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-KTT39KP:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataFram</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1adff749940>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ddf6430-03e6-4c15-8be4-aad326e31937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- levy: string (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- prod_year: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- leather_interior: string (nullable = true)\n",
      " |-- fuel_type: string (nullable = true)\n",
      " |-- engine_volume: string (nullable = true)\n",
      " |-- mileage: string (nullable = true)\n",
      " |-- cylinders: double (nullable = true)\n",
      " |-- gear_box_type: string (nullable = true)\n",
      " |-- drive_wheels: string (nullable = true)\n",
      " |-- doors: string (nullable = true)\n",
      " |-- wheel: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- airbags: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=45654403, price=13328, levy='1399', manufacturer='LEXUS', model='RX 450', prod_year=2010, category='Jeep', leather_interior='Yes', fuel_type='Hybrid', engine_volume='3.5', mileage='186005 km', cylinders=6.0, gear_box_type='Automatic', drive_wheels='4x4', doors='04-May', wheel='Left wheel', color='Silver', airbags=12),\n",
       " Row(id=44731507, price=16621, levy='1018', manufacturer='CHEVROLET', model='Equinox', prod_year=2011, category='Jeep', leather_interior='No', fuel_type='Petrol', engine_volume='3', mileage='192000 km', cylinders=6.0, gear_box_type='Tiptronic', drive_wheels='4x4', doors='04-May', wheel='Left wheel', color='Black', airbags=8),\n",
       " Row(id=45774419, price=8467, levy='-', manufacturer='HONDA', model='FIT', prod_year=2006, category='Hatchback', leather_interior='No', fuel_type='Petrol', engine_volume='1.3', mileage='200000 km', cylinders=4.0, gear_box_type='Variator', drive_wheels='Front', doors='04-May', wheel='Right-hand drive', color='Black', airbags=2),\n",
       " Row(id=45769185, price=3607, levy='862', manufacturer='FORD', model='Escape', prod_year=2011, category='Jeep', leather_interior='Yes', fuel_type='Hybrid', engine_volume='2.5', mileage='168966 km', cylinders=4.0, gear_box_type='Automatic', drive_wheels='4x4', doors='04-May', wheel='Left wheel', color='White', airbags=0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read DataSet\n",
    "\n",
    "df = spark.read.option('header', 'true').csv('train.csv', inferSchema = True)\n",
    "#show() para verlo\n",
    "df.printSchema()\n",
    "#dtypes\n",
    "\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b4cdf6b-21c7-4fed-8e5e-bf8472983cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- levy: string (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- prod_year: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- leather_interior: string (nullable = true)\n",
      " |-- fuel_type: string (nullable = true)\n",
      " |-- engine_volume: string (nullable = true)\n",
      " |-- mileage: integer (nullable = true)\n",
      " |-- cylinders: double (nullable = true)\n",
      " |-- gear_box_type: string (nullable = true)\n",
      " |-- drive_wheels: string (nullable = true)\n",
      " |-- doors: string (nullable = true)\n",
      " |-- wheel: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- airbags: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Change format of colum\n",
    "df = df.withColumn(\n",
    "    \"mileage\", \n",
    "    regexp_replace(col(\"mileage\"), \" km\", \"\").cast(\"int\")\n",
    ")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c803a01-85d4-46b8-8711-cf855cd3aa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- prod_year: integer (nullable = true)\n",
      " |-- mileage: integer (nullable = true)\n",
      " |-- cylinders: double (nullable = true)\n",
      " |-- airbags: integer (nullable = true)\n",
      " |-- levyI: double (nullable = false)\n",
      " |-- manufacturerI: double (nullable = false)\n",
      " |-- modelI: double (nullable = false)\n",
      " |-- categoryI: double (nullable = false)\n",
      " |-- leather_interiorI: double (nullable = false)\n",
      " |-- fuel_typeI: double (nullable = false)\n",
      " |-- engine_volumeI: double (nullable = false)\n",
      " |-- gear_box_typeI: double (nullable = false)\n",
      " |-- drive_wheelsI: double (nullable = false)\n",
      " |-- doorsI: double (nullable = false)\n",
      " |-- wheelI: double (nullable = false)\n",
      " |-- colorI: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Colums will be deleted\n",
    "columns_to_drop = []\n",
    "\n",
    "\n",
    "for column_name, column_type in df.dtypes:\n",
    "    \n",
    "    # Check is colum is string\n",
    "    if column_type == \"string\":\n",
    "        # New name indexed colum\n",
    "        indexed_column_name = f\"{column_name}I\"\n",
    "        \n",
    "        # Create StringIndexer\n",
    "        indexer = StringIndexer(inputCol=column_name, outputCol=indexed_column_name)\n",
    "        \n",
    "        # Apply StringIndexer to DataFrame\n",
    "        df = indexer.fit(df).transform(df)\n",
    "        \n",
    "        # Add colum to list\n",
    "        columns_to_drop.append(column_name)\n",
    "\n",
    "# Delete old columns\n",
    "df = df.drop(*columns_to_drop)\n",
    "\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0860fab0-ce4b-4711-9897-c1ea31fee479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- prod_year: integer (nullable = true)\n",
      " |-- mileage: integer (nullable = true)\n",
      " |-- cylinders: double (nullable = true)\n",
      " |-- airbags: integer (nullable = true)\n",
      " |-- levyI: double (nullable = false)\n",
      " |-- manufacturerI: double (nullable = false)\n",
      " |-- modelI: double (nullable = false)\n",
      " |-- categoryI: double (nullable = false)\n",
      " |-- leather_interiorI: double (nullable = false)\n",
      " |-- fuel_typeI: double (nullable = false)\n",
      " |-- engine_volumeI: double (nullable = false)\n",
      " |-- gear_box_typeI: double (nullable = false)\n",
      " |-- drive_wheelsI: double (nullable = false)\n",
      " |-- doorsI: double (nullable = false)\n",
      " |-- wheelI: double (nullable = false)\n",
      " |-- colorI: double (nullable = false)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Moving price colum\n",
    "\n",
    "cols = [col for col in df.columns if col != \"price\"] + [\"price\"]\n",
    "df = df.select(*cols)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8750f932-c647-4a41-afb9-8bf56ec4b635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0.9935280230288475, prod_year=0.8765432098765431, mileage=8.661532778600945e-05, cylinders=0.3333333333333333, airbags=0.75, levyI=0.02867383512544803, manufacturerI=0.09375, modelI=0.013845185651353053, categoryI=0.1, leather_interiorI=0.0, fuel_typeI=0.3333333333333333, engine_volumeI=0.04716981132075471, gear_box_typeI=0.0, drive_wheelsI=0.5, doorsI=0.0, wheelI=0.0, colorI=0.13333333333333333, price=0.0005065855937122719),\n",
       " Row(id=0.956714926907598, prod_year=0.8888888888888888, mileage=8.94069672047193e-05, cylinders=0.3333333333333333, airbags=0.5, levyI=0.04659498207885305, manufacturerI=0.0625, modelI=0.07048458149779736, categoryI=0.1, leather_interiorI=1.0, fuel_typeI=0.0, engine_volumeI=0.0660377358490566, gear_box_typeI=0.3333333333333333, drive_wheelsI=0.5, doorsI=0.0, wheelI=0.0, colorI=0.0, price=0.0006317590281006948),\n",
       " Row(id=0.9983153019249396, prod_year=0.8271604938271604, mileage=9.313225750491594e-05, cylinders=0.2, airbags=0.125, levyI=0.0, manufacturerI=0.109375, modelI=0.0037759597230962874, categoryI=0.2, leather_interiorI=1.0, fuel_typeI=0.0, engine_volumeI=0.07547169811320754, gear_box_typeI=1.0, drive_wheelsI=0.0, doorsI=0.0, wheelI=1.0, colorI=0.0, price=0.0003218093821841445)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Join all columns in vector\n",
    "assembler = VectorAssembler(inputCols=df.columns, outputCol=\"features\")\n",
    "df_vector = assembler.transform(df)\n",
    "\n",
    "# Apply MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(df_vector)\n",
    "df_scaled = scaler_model.transform(df_vector)\n",
    "\n",
    "\n",
    "# Break the scaled vector into individual columns\n",
    "df_normalized = df_scaled.withColumn(\"scaled_array\", vector_to_array(\"scaled_features\"))\n",
    "\n",
    "\n",
    "# Create new columns for new normalized values\n",
    "for idx, col_name in enumerate(df.columns):\n",
    "    df_normalized = df_normalized.withColumn(f\"{col_name}\", col(\"scaled_array\")[idx])\n",
    "\n",
    "# Paso 4: Select only normalized colums\n",
    "scaled_columns = [f\"{col}\" for col in df.columns]\n",
    "df_final = df_normalized.select(*[col(c).alias(c) for c in scaled_columns])\n",
    "\n",
    "# Mostrar resultado\n",
    "df_final.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "578bed96-e8db-424e-97d6-5d397abbe22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- prod_year: double (nullable = true)\n",
      " |-- mileage: double (nullable = true)\n",
      " |-- cylinders: double (nullable = true)\n",
      " |-- airbags: double (nullable = true)\n",
      " |-- levyI: double (nullable = true)\n",
      " |-- manufacturerI: double (nullable = true)\n",
      " |-- modelI: double (nullable = true)\n",
      " |-- categoryI: double (nullable = true)\n",
      " |-- leather_interiorI: double (nullable = true)\n",
      " |-- fuel_typeI: double (nullable = true)\n",
      " |-- engine_volumeI: double (nullable = true)\n",
      " |-- gear_box_typeI: double (nullable = true)\n",
      " |-- drive_wheelsI: double (nullable = true)\n",
      " |-- doorsI: double (nullable = true)\n",
      " |-- wheelI: double (nullable = true)\n",
      " |-- colorI: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Eliminar la columna \"id\"\n",
    "df= df_final.drop(\"id\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "915115d0-5717-44f9-afb7-9001c8db7f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9597\n",
      "9640\n"
     ]
    }
   ],
   "source": [
    "# Slpit de dataset in train and test\n",
    "train_df, test_df = df.randomSplit([0.5, 0.5], seed=1)\n",
    "\n",
    "print(train_df.count())\n",
    "\n",
    "print(test_df.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b2ab168-c2b3-4e31-8d74-f10a700af4b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/C:/Users/Fernando/Universidad/Asignaturas Montenegro/ISS-AI/src/Spark/train_updated.csv already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_updated.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Problem I think I need HADOOP_HOME\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#https://github.com/steveloughran/winutils\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1863\u001b[0m )\n\u001b[1;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/C:/Users/Fernando/Universidad/Asignaturas Montenegro/ISS-AI/src/Spark/train_updated.csv already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "train_df.write.csv(\"train_updated.csv\",header = True)\n",
    "test_df.write.csv(\"test_updated.csv\",header = True)\n",
    "\n",
    "#Problem I think I need HADOOP_HOME\n",
    "#https://github.com/steveloughran/winutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20dce7d-7e00-49d4-a8d2-ac4d1ef8677d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
